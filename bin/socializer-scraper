#!/usr/bin/env ruby

require 'pry'
require 'yaml'
require 'thor'
require 'fileutils'
require 'socializer/scraper'
STDOUT.sync = true

class Socializer::Scraper::CLI < Thor

  desc "emails [URLs]", "scrape emails for a given URL and all subsequently found URLs"
  method_options pattern: :string, desc: "Comma separate list of patterns that selects which links to follow"
  def emails(*urls)
    extractor = Socializer::Scraper::Extractor.new collectors: [:email]
    urls.each do |website|

      puts "=" * 100
      puts "Current Time is : #{Time.now.utc}"
      puts "Scraping website: #{website}"
      puts "=" * 100

      website = URI.parse(website.start_with?("http") ? website : "http://#{website}")
      file = File.join(Dir.pwd, "#{website.host}.yml")
      counter, list = 0, (File.exists?(file) ? YAML.load_file(file) : [])

      patterns = options.has_key?("pattern") ? options["pattern"].split(",").map{|a| Regexp.new a} : []

      extractor.url = website.to_s
      extractor.run(*patterns) do |page, collector, found|
        found  = found.map{ |email| email.strip.downcase }.accumulate - list
        list  |= found

        found = found.count
        found = "." if found.to_i < 1
        found = "+" if found.to_i > 9

        if counter % 100 == 99
          File.open(file, "w") { |f| f.puts list.to_yaml }
          STDOUT.puts found
        else
          STDOUT.print found
        end

        counter += 1
      end

      puts "=" * 100
      puts "Finish Time is  : #{Time.now.utc}"
      puts "Emails Found    : #{list.count}"
    end
  end

end

Socializer::Scraper::CLI.start ARGV
